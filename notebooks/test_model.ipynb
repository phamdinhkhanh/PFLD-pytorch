{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input channel: 160. Output channel: 960\nSize after conv_steem: torch.Size([1, 16, 112, 112])\nAfter remaiing blocks:  torch.Size([1, 960, 14, 14])\nTime:  0.048256635665893555\nShape fea:  torch.Size([1, 80, 14, 14])\nRemainig X:  torch.Size([1, 196])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "60"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../models\")\n",
    "from pfld import InvertedResidual\n",
    "from ghostnet import GhostBottleneck, GhostModule, GhostNet, ghostnet, _make_divisible, ConvBnAct\n",
    "from pfld import PFLDInference, conv_bn\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# cfgs = [\n",
    "#         # k, t, c, SE, s \n",
    "#         # stage1\n",
    "#         [[3,  16,  16, 0, 2]],\n",
    "#         # stage2\n",
    "#         [[3,  48,  24, 0, 1]],\n",
    "#         [[3,  72,  24, 0, 1]],\n",
    "#         # stage3\n",
    "#         [[5,  72,  40, 0.25, 1]],\n",
    "#         [[5, 120,  40, 0.25, 1]],\n",
    "#         # stage4\n",
    "#         [[3, 240,  80, 0, 1]], #The original number of channels here is 80, but I change to 64 so that it fit to the AuxiliaryNet\n",
    "#         [[3, 200,  80, 0, 2],\n",
    "#          [3, 184,  80, 0, 1],\n",
    "#          [3, 184,  80, 0, 1],\n",
    "#          [3, 480, 112, 0.25, 1],\n",
    "#          [3, 672, 112, 0.25, 1]\n",
    "#         ],\n",
    "#         # stage5\n",
    "#         [[5, 672, 160, 0.25, 1]],\n",
    "#         [[5, 960, 160, 0, 1],\n",
    "#          [5, 960, 160, 0.25, 1],\n",
    "#          [5, 960, 160, 0, 1],\n",
    "#          [5, 960, 160, 0.25, 1]\n",
    "#         ],\n",
    "\n",
    "#         # final\n",
    "#         # [[5, 320, 16, 0.25, 1]]\n",
    "#     ]\n",
    "\n",
    "# cfgs = [\n",
    "#     # k, t,    c,se,s \n",
    "#     [[3, 64*2, 64, 0, 2]],\n",
    "\n",
    "#     [[3, 64*2, 64, 0.25,1],\n",
    "#      [3, 64*2, 64, 0.25,1],\n",
    "#      [3, 64*2, 64, 0.25,1],\n",
    "#      [3, 64*2, 64, 0.25,1]\n",
    "#     ],\n",
    "\n",
    "#     [[3, 64*2, 128, 0, 2]],\n",
    "\n",
    "#     [[3, 128*4, 128, 0, 1],\n",
    "#      [3, 128*4, 128, 0.25, 1],\n",
    "#      [3, 128*4, 128, 0.25, 1],\n",
    "#      [3, 128*4, 128, 0.25, 1],\n",
    "#      [3, 128*4, 128, 0.25, 1],\n",
    "#      [3, 128*4, 128, 0.25, 1],\n",
    "#      [3, 128*2, 16, 0, 1],\n",
    "#     ]\n",
    "\n",
    "# ]\n",
    "\n",
    "\n",
    "cfgs = [\n",
    "        # k, t, c, SE, s \n",
    "        # stage1\n",
    "        [[3,  16,  16, 0, 1],\n",
    "         [3,  48,  24, 0, 2], #56x56\n",
    "        ],\n",
    "\n",
    "        # stage 2\n",
    "        [[3,  72,  24, 0, 1],\n",
    "         [5,  72,  40, 0.25, 2] # 28x28\n",
    "        ],\n",
    "\n",
    "        # stage 3\n",
    "        [[5, 120,  40, 0.25, 1],\n",
    "         [3, 240,  80, 0, 2]  #14x14\n",
    "        ],\n",
    "\n",
    "        # stage 4\n",
    "        [[3, 200,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 480, 112, 0.25, 1],\n",
    "         [3, 672, 112, 0.25, 1],\n",
    "         [5, 672, 160, 0.25, 1]\n",
    "        ],\n",
    "\n",
    "        # stage5\n",
    "        [[5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1],\n",
    "         [5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1]\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "class CustomizedGhostNet(nn.Module):\n",
    "    def __init__(self, cfgs, width=1.0, dropout=0.2):\n",
    "        super(CustomizedGhostNet, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # building first layer\n",
    "        output_channel = _make_divisible(16 * width, 4)\n",
    "        self.conv_stem = nn.Conv2d(3, output_channel, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channel)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        input_channel = output_channel\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        first_6_stages = []  # This one used for another branch\n",
    "        remaining_stages = []\n",
    "        stages = []\n",
    "        block = GhostBottleneck\n",
    "        for i, cfg in enumerate(self.cfgs):\n",
    "            layers = []\n",
    "            for k, exp_size, c, se_ratio, s in cfg:\n",
    "                output_channel = _make_divisible(c * width, 4)\n",
    "                hidden_channel = _make_divisible(exp_size * width, 4)\n",
    "                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n",
    "                            se_ratio=se_ratio))\n",
    "                input_channel = output_channel\n",
    "\n",
    "            if i<=1:\n",
    "                first_6_stages.append(nn.Sequential(*layers))\n",
    "            else:\n",
    "                remaining_stages.append(nn.Sequential(*layers))\n",
    "                \n",
    "\n",
    "        output_channel = _make_divisible(exp_size * width, 4)\n",
    "        # output_channel = 16\n",
    "        print(f\"Input channel: {input_channel}. Output channel: {output_channel}\")\n",
    "        remaining_stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n",
    "        \n",
    "        self.begining_blocks = nn.Sequential(*first_6_stages)\n",
    "        self.remaining_blocks = nn.Sequential(*remaining_stages)  # 960x14x14\n",
    "        self.conv6 = ConvBnAct(output_channel, 16, 1)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv7 = conv_bn(16, 32, 3, 2)  # [32, 7, 7]\n",
    "        self.conv8 = nn.Conv2d(32, 128, 7, 1, 0)  # [128, 1, 1]\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.avg_pool1 = nn.AvgPool2d(14)\n",
    "        self.avg_pool2 = nn.AvgPool2d(7)\n",
    "        self.fc = nn.Linear(176, 196)\n",
    "\n",
    "        # building last several layers\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        print(\"Size after conv_steem:\", x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        features_for_auxiliarynet = self.begining_blocks(x)\n",
    "        x = self.remaining_blocks(features_for_auxiliarynet)\n",
    "        print(\"After remaiing blocks: \", x.shape)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x1 = self.avg_pool1(x)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x2 = self.avg_pool2(x)\n",
    "\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "\n",
    "        x3 = self.relu(self.conv8(x))\n",
    "\n",
    "        x3 = x3.view(x1.size(0), -1)\n",
    "\n",
    "        multi_scale = torch.cat([x1, x2, x3], 1)\n",
    "        landmarks = self.fc(multi_scale)\n",
    "    \n",
    "        return features_for_auxiliarynet, landmarks\n",
    "\n",
    "\n",
    "model = CustomizedGhostNet(cfgs, width=1)\n",
    "model.eval()\n",
    "input = torch.randn(1,3,112,112)\n",
    "t1 = time.time()\n",
    "fea, x = model(input)\n",
    "print(\"Time: \", time.time()-t1)\n",
    "print(\"Shape fea: \", fea.shape)\n",
    "print(\"Remainig X: \", x.shape)\n",
    "\n",
    "# len(model.state_dict().keys())\n",
    "b = list(model.state_dict().keys())\n",
    "\n",
    "b1 = list(filter(lambda i: \"shortcut\" in i , b))\n",
    "len(b1)\n",
    "# # len(b1)\n",
    "# for c,i in enumerate(b):\n",
    "#   print(i)\n",
    "#   if c >=210:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = torch.load(\"../checkpoint_imagenet/state_dict_93.98.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()[\"begining_blocks.0.0.ghost1.primary_conv.0.weight\"].data.copy_(ck[\"blocks.0.0.ghost1.primary_conv.0.weight\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "60"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "a = list(ck.keys())\n",
    "a1 = list(filter(lambda i: \"shortcut\" in i ,a))\n",
    "len(a1)\n",
    "# for c, i in enumerate(a):\n",
    "#     print(i)\n",
    "#     if c>=200:\n",
    "#         break\n",
    " \n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'blocks.7.0.se.conv_expand.weight'"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "def find_corresponding_layer(l):\n",
    "    map_dict  = {\n",
    "    \"begining_blocks.0\" : \"blocks.0\",\n",
    "    \"begining_blocks.1\" : \"blocks.1\",\n",
    "    \"begining_blocks.2\" : \"blocks.2\",\n",
    "    \"begining_blocks.3\" : \"blocks.3\",\n",
    "    \"begining_blocks.4\" : \"blocks.4\",\n",
    "    \"begining_blocks.5\" : \"blocks.5\",\n",
    "    \"remaining_blocks.0\" : \"blocks.6\",\n",
    "    \"remaining_blocks.1\" : \"blocks.7\",\n",
    "    \"remaining_blocks.2\" : \"blocks.8\",\n",
    "    \"remaining_blocks.3\" : \"blocks.9\",\n",
    "    }\n",
    "\n",
    "    for k,v in map_dict.items():\n",
    "        l = l.replace(k,v)\n",
    "\n",
    "    return l    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "our_model_layer_keys = list(model.state_dict().keys())\n",
    "pretrained_layer_keys = list(ck.keys())\n",
    "\n",
    "for l in  our_model_layer_keys:\n",
    "    l1 = find_corresponding_layer(l)\n",
    "    if l1 in pretrained_layer_keys:\n",
    "        if ck[l1].data.shape == model.state_dict()[l].data.shape:\n",
    "            \n",
    "        else:\n",
    "            print(f\"{l}. shape:{model.state_dict()[l].data.shape}. {l1}. Shape: {ck[l1].data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "535\n512\nconv_stem.weight       ----------------       conv_stem.weight\nbn1.weight       ----------------       bn1.weight\nbn1.bias       ----------------       bn1.bias\nbn1.running_mean       ----------------       bn1.running_mean\nbn1.running_var       ----------------       bn1.running_var\nbn1.num_batches_tracked       ----------------       bn1.num_batches_tracked\nbegining_blocks.0.0.conv_dw.weight       ----------------       blocks.1.0.conv_dw.weight\nbegining_blocks.0.0.bn_dw.weight       ----------------       blocks.1.0.bn_dw.weight\nbegining_blocks.0.0.bn_dw.bias       ----------------       blocks.1.0.bn_dw.bias\nbegining_blocks.0.0.bn_dw.running_mean       ----------------       blocks.1.0.bn_dw.running_mean\nbegining_blocks.0.0.bn_dw.running_var       ----------------       blocks.1.0.bn_dw.running_var\nbegining_blocks.0.0.bn_dw.num_batches_tracked       ----------------       blocks.1.0.bn_dw.num_batches_tracked\nbegining_blocks.0.0.shortcut.0.weight       ----------------       blocks.1.0.shortcut.0.weight\nbegining_blocks.0.0.shortcut.1.weight       ----------------       blocks.1.0.shortcut.1.weight\nbegining_blocks.0.0.shortcut.1.bias       ----------------       blocks.1.0.shortcut.1.bias\nbegining_blocks.0.0.shortcut.1.running_mean       ----------------       blocks.1.0.shortcut.1.running_mean\nbegining_blocks.0.0.shortcut.1.running_var       ----------------       blocks.1.0.shortcut.1.running_var\nbegining_blocks.0.0.shortcut.1.num_batches_tracked       ----------------       blocks.1.0.shortcut.1.num_batches_tracked\nbegining_blocks.0.0.shortcut.2.weight       ----------------       blocks.1.0.shortcut.2.weight\nbegining_blocks.0.0.shortcut.3.weight       ----------------       blocks.1.0.shortcut.3.weight\nbegining_blocks.0.0.shortcut.3.bias       ----------------       blocks.1.0.shortcut.3.bias\nbegining_blocks.0.0.shortcut.3.running_mean       ----------------       blocks.1.0.shortcut.3.running_mean\nbegining_blocks.0.0.shortcut.3.running_var       ----------------       blocks.1.0.shortcut.3.running_var\nbegining_blocks.0.0.shortcut.3.num_batches_tracked       ----------------       blocks.1.0.shortcut.3.num_batches_tracked\nbegining_blocks.1.0.shortcut.0.weight       ----------------       blocks.3.0.conv_dw.weight\nbegining_blocks.1.0.shortcut.1.weight       ----------------       blocks.3.0.bn_dw.weight\nbegining_blocks.1.0.shortcut.1.bias       ----------------       blocks.3.0.bn_dw.bias\nbegining_blocks.1.0.shortcut.1.running_mean       ----------------       blocks.3.0.bn_dw.running_mean\nbegining_blocks.1.0.shortcut.1.running_var       ----------------       blocks.3.0.bn_dw.running_var\nbegining_blocks.1.0.shortcut.1.num_batches_tracked       ----------------       blocks.3.0.bn_dw.num_batches_tracked\nbegining_blocks.1.0.shortcut.2.weight       ----------------       blocks.3.0.se.conv_reduce.weight\nbegining_blocks.1.0.shortcut.3.weight       ----------------       blocks.3.0.se.conv_reduce.bias\nbegining_blocks.1.0.shortcut.3.bias       ----------------       blocks.3.0.se.conv_expand.weight\nbegining_blocks.1.0.shortcut.3.running_mean       ----------------       blocks.3.0.se.conv_expand.bias\nbegining_blocks.1.0.shortcut.3.running_var       ----------------       blocks.3.0.shortcut.0.weight\nbegining_blocks.1.0.shortcut.3.num_batches_tracked       ----------------       blocks.3.0.shortcut.1.weight\nbegining_blocks.3.0.se.conv_reduce.weight       ----------------       blocks.3.0.shortcut.1.bias\nbegining_blocks.3.0.se.conv_reduce.bias       ----------------       blocks.3.0.shortcut.1.running_mean\nbegining_blocks.3.0.se.conv_expand.weight       ----------------       blocks.3.0.shortcut.1.running_var\nbegining_blocks.3.0.se.conv_expand.bias       ----------------       blocks.3.0.shortcut.1.num_batches_tracked\nbegining_blocks.3.0.shortcut.0.weight       ----------------       blocks.3.0.shortcut.2.weight\nbegining_blocks.3.0.shortcut.1.weight       ----------------       blocks.3.0.shortcut.3.weight\nbegining_blocks.3.0.shortcut.1.bias       ----------------       blocks.3.0.shortcut.3.bias\nbegining_blocks.3.0.shortcut.1.running_mean       ----------------       blocks.3.0.shortcut.3.running_mean\nbegining_blocks.3.0.shortcut.1.running_var       ----------------       blocks.3.0.shortcut.3.running_var\nbegining_blocks.3.0.shortcut.1.num_batches_tracked       ----------------       blocks.3.0.shortcut.3.num_batches_tracked\nbegining_blocks.3.0.shortcut.2.weight       ----------------       blocks.4.0.se.conv_reduce.weight\nbegining_blocks.3.0.shortcut.3.weight       ----------------       blocks.4.0.se.conv_reduce.bias\nbegining_blocks.3.0.shortcut.3.bias       ----------------       blocks.4.0.se.conv_expand.weight\nbegining_blocks.3.0.shortcut.3.running_mean       ----------------       blocks.4.0.se.conv_expand.bias\nbegining_blocks.3.0.shortcut.3.running_var       ----------------       blocks.5.0.conv_dw.weight\nbegining_blocks.3.0.shortcut.3.num_batches_tracked       ----------------       blocks.5.0.bn_dw.weight\nbegining_blocks.4.0.se.conv_reduce.weight       ----------------       blocks.5.0.bn_dw.bias\nbegining_blocks.4.0.se.conv_reduce.bias       ----------------       blocks.5.0.bn_dw.running_mean\nbegining_blocks.4.0.se.conv_expand.weight       ----------------       blocks.5.0.bn_dw.running_var\nbegining_blocks.4.0.se.conv_expand.bias       ----------------       blocks.5.0.bn_dw.num_batches_tracked\nbegining_blocks.5.0.shortcut.0.weight       ----------------       blocks.5.0.shortcut.0.weight\nbegining_blocks.5.0.shortcut.1.weight       ----------------       blocks.5.0.shortcut.1.weight\nbegining_blocks.5.0.shortcut.1.bias       ----------------       blocks.5.0.shortcut.1.bias\nbegining_blocks.5.0.shortcut.1.running_mean       ----------------       blocks.5.0.shortcut.1.running_mean\nbegining_blocks.5.0.shortcut.1.running_var       ----------------       blocks.5.0.shortcut.1.running_var\nbegining_blocks.5.0.shortcut.1.num_batches_tracked       ----------------       blocks.5.0.shortcut.1.num_batches_tracked\nbegining_blocks.5.0.shortcut.2.weight       ----------------       blocks.5.0.shortcut.2.weight\nbegining_blocks.5.0.shortcut.3.weight       ----------------       blocks.5.0.shortcut.3.weight\nbegining_blocks.5.0.shortcut.3.bias       ----------------       blocks.5.0.shortcut.3.bias\nbegining_blocks.5.0.shortcut.3.running_mean       ----------------       blocks.5.0.shortcut.3.running_mean\nbegining_blocks.5.0.shortcut.3.running_var       ----------------       blocks.5.0.shortcut.3.running_var\nbegining_blocks.5.0.shortcut.3.num_batches_tracked       ----------------       blocks.5.0.shortcut.3.num_batches_tracked\nremaining_blocks.0.0.conv_dw.weight       ----------------       blocks.6.3.se.conv_reduce.weight\nremaining_blocks.0.0.bn_dw.weight       ----------------       blocks.6.3.se.conv_reduce.bias\nremaining_blocks.0.0.bn_dw.bias       ----------------       blocks.6.3.se.conv_expand.weight\nremaining_blocks.0.0.bn_dw.running_mean       ----------------       blocks.6.3.se.conv_expand.bias\nremaining_blocks.0.0.bn_dw.running_var       ----------------       blocks.6.3.shortcut.0.weight\nremaining_blocks.0.0.bn_dw.num_batches_tracked       ----------------       blocks.6.3.shortcut.1.weight\nremaining_blocks.0.0.shortcut.0.weight       ----------------       blocks.6.3.shortcut.1.bias\nremaining_blocks.0.0.shortcut.1.weight       ----------------       blocks.6.3.shortcut.1.running_mean\nremaining_blocks.0.0.shortcut.1.bias       ----------------       blocks.6.3.shortcut.1.running_var\nremaining_blocks.0.0.shortcut.1.running_mean       ----------------       blocks.6.3.shortcut.1.num_batches_tracked\nremaining_blocks.0.0.shortcut.1.running_var       ----------------       blocks.6.3.shortcut.2.weight\nremaining_blocks.0.0.shortcut.1.num_batches_tracked       ----------------       blocks.6.3.shortcut.3.weight\nremaining_blocks.0.0.shortcut.2.weight       ----------------       blocks.6.3.shortcut.3.bias\nremaining_blocks.0.0.shortcut.3.weight       ----------------       blocks.6.3.shortcut.3.running_mean\nremaining_blocks.0.0.shortcut.3.bias       ----------------       blocks.6.3.shortcut.3.running_var\nremaining_blocks.0.0.shortcut.3.running_mean       ----------------       blocks.6.3.shortcut.3.num_batches_tracked\nremaining_blocks.0.0.shortcut.3.running_var       ----------------       blocks.6.4.se.conv_reduce.weight\nremaining_blocks.0.0.shortcut.3.num_batches_tracked       ----------------       blocks.6.4.se.conv_reduce.bias\nremaining_blocks.0.3.se.conv_reduce.weight       ----------------       blocks.6.4.se.conv_expand.weight\nremaining_blocks.0.3.se.conv_reduce.bias       ----------------       blocks.6.4.se.conv_expand.bias\nremaining_blocks.0.3.se.conv_expand.weight       ----------------       blocks.7.0.conv_dw.weight\nremaining_blocks.0.3.se.conv_expand.bias       ----------------       blocks.7.0.bn_dw.weight\nremaining_blocks.0.3.shortcut.0.weight       ----------------       blocks.7.0.bn_dw.bias\nremaining_blocks.0.3.shortcut.1.weight       ----------------       blocks.7.0.bn_dw.running_mean\nremaining_blocks.0.3.shortcut.1.bias       ----------------       blocks.7.0.bn_dw.running_var\nremaining_blocks.0.3.shortcut.1.running_mean       ----------------       blocks.7.0.bn_dw.num_batches_tracked\nremaining_blocks.0.3.shortcut.1.running_var       ----------------       blocks.7.0.se.conv_reduce.weight\nremaining_blocks.0.3.shortcut.1.num_batches_tracked       ----------------       blocks.7.0.se.conv_reduce.bias\nremaining_blocks.0.3.shortcut.2.weight       ----------------       blocks.7.0.se.conv_expand.weight\nremaining_blocks.0.3.shortcut.3.weight       ----------------       blocks.7.0.se.conv_expand.bias\nremaining_blocks.0.3.shortcut.3.bias       ----------------       blocks.7.0.shortcut.0.weight\nremaining_blocks.0.3.shortcut.3.running_mean       ----------------       blocks.7.0.shortcut.1.weight\nremaining_blocks.0.3.shortcut.3.running_var       ----------------       blocks.7.0.shortcut.1.bias\nremaining_blocks.0.3.shortcut.3.num_batches_tracked       ----------------       blocks.7.0.shortcut.1.running_mean\nremaining_blocks.0.4.se.conv_reduce.weight       ----------------       blocks.7.0.shortcut.1.running_var\nremaining_blocks.0.4.se.conv_reduce.bias       ----------------       blocks.7.0.shortcut.1.num_batches_tracked\nremaining_blocks.0.4.se.conv_expand.weight       ----------------       blocks.7.0.shortcut.2.weight\nremaining_blocks.0.4.se.conv_expand.bias       ----------------       blocks.7.0.shortcut.3.weight\nremaining_blocks.1.0.se.conv_reduce.weight       ----------------       blocks.7.0.shortcut.3.bias\nremaining_blocks.1.0.se.conv_reduce.bias       ----------------       blocks.7.0.shortcut.3.running_mean\nremaining_blocks.1.0.se.conv_expand.weight       ----------------       blocks.7.0.shortcut.3.running_var\nremaining_blocks.1.0.se.conv_expand.bias       ----------------       blocks.7.0.shortcut.3.num_batches_tracked\nremaining_blocks.1.0.shortcut.0.weight       ----------------       blocks.8.1.se.conv_reduce.weight\nremaining_blocks.1.0.shortcut.1.weight       ----------------       blocks.8.1.se.conv_reduce.bias\nremaining_blocks.1.0.shortcut.1.bias       ----------------       blocks.8.1.se.conv_expand.weight\nremaining_blocks.1.0.shortcut.1.running_mean       ----------------       blocks.8.1.se.conv_expand.bias\nremaining_blocks.1.0.shortcut.1.running_var       ----------------       blocks.8.3.se.conv_reduce.weight\nremaining_blocks.1.0.shortcut.1.num_batches_tracked       ----------------       blocks.8.3.se.conv_reduce.bias\nremaining_blocks.1.0.shortcut.2.weight       ----------------       blocks.8.3.se.conv_expand.weight\nremaining_blocks.1.0.shortcut.3.weight       ----------------       blocks.8.3.se.conv_expand.bias\nremaining_blocks.1.0.shortcut.3.bias       ----------------       blocks.9.0.conv.weight\nremaining_blocks.1.0.shortcut.3.running_mean       ----------------       blocks.9.0.bn1.weight\nremaining_blocks.1.0.shortcut.3.running_var       ----------------       blocks.9.0.bn1.bias\nremaining_blocks.1.0.shortcut.3.num_batches_tracked       ----------------       blocks.9.0.bn1.running_mean\nremaining_blocks.2.1.se.conv_reduce.weight       ----------------       blocks.9.0.bn1.running_var\nremaining_blocks.2.1.se.conv_reduce.bias       ----------------       blocks.9.0.bn1.num_batches_tracked\nremaining_blocks.2.1.se.conv_expand.weight       ----------------       conv_head.weight\nremaining_blocks.2.1.se.conv_expand.bias       ----------------       conv_head.bias\nremaining_blocks.2.3.se.conv_reduce.weight       ----------------       classifier.weight\nremaining_blocks.2.3.se.conv_reduce.bias       ----------------       classifier.bias\n"
    }
   ],
   "source": [
    "print(len(model.state_dict().keys()))\n",
    "print(len(ck.keys()))\n",
    "\n",
    "a = list(model.state_dict().keys())\n",
    "b = list(ck.keys())\n",
    "a = list(filter(lambda i: \"ghost\" not in i ,a))\n",
    "b = list(filter(lambda i: \"ghost\" not in i ,b))\n",
    "\n",
    "a1 = list(map(lambda i: \".\".join(i.split(\".\")[3:]) ,a))\n",
    "b1 = list(map(lambda i: \".\".join(i.split(\".\")[3:]) ,b))\n",
    "\n",
    "# a1 = list(set(a1))\n",
    "for r, (i, j) in enumerate(zip(a, b)):\n",
    "    if r <600:  \n",
    "        print(i, \"      ----------------      \", j)\n",
    "        # print(model.state_dict()[i].data.shape, \"      ----------------      \", ck[j].data.shape)\n",
    "        # print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "j = 0 # index of pretrain model\n",
    "for l in a:\n",
    "    l1 = \".\".join(l.split(\".\")[3:])\n",
    "    \n",
    "    for k in range(j, len(b)):\n",
    "        k1 = \".\".join(b[j].split(\".\")[3:])\n",
    "        if l1==k1 and l1!=\"\":\n",
    "            print(l1, \"----------------\", k1)\n",
    "            print(model.state_dict()[l].shape,\"-----------\", ck[b[k]].shape)\n",
    "            \n",
    "            break\n",
    "        j = j+1\n",
    "\n",
    "    # if r<200:\n",
    "        # print(model.state_dict()[i].data.shape, \"----------\", ck[j].data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Feature after ghotnet block:  torch.Size([1, 960, 4, 4])\nFeature after global pool:  torch.Size([1, 960, 1, 1])\nFeature after conv_head:  torch.Size([1, 1280, 1, 1])\nTime inference one input: 0.038233280181884766\ntorch.Size([1, 1000])\nOut1 shape: torch.Size([1, 64, 28, 28])\nX1 shape: torch.Size([1, 16, 1, 1])\nX2 shape: torch.Size([1, 32, 1, 1])\nX3 shape: torch.Size([1, 128, 1, 1])\nTime:  0.03032398223876953\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = ghostnet(width=1)\n",
    "model.eval()\n",
    "# print(model)\n",
    "input = torch.randn(1,3,112,112)\n",
    "\n",
    "t1 = time.time()\n",
    "y = model(input)\n",
    "print(\"Time inference one input:\", time.time()-t1)\n",
    "print(y.size())\n",
    "\n",
    "input = torch.randn(1, 3, 112, 112)\n",
    "plfd_backbone = PFLDInference()\n",
    "plfd_backbone.eval()\n",
    "t1 = time.time()\n",
    "features, landmarks = plfd_backbone(input)\n",
    "print(\"Time: \", time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual\n",
    "w=64\n",
    "h=64\n",
    "stride=2\n",
    "expand_ratio=1\n",
    "res = InvertedResidual(w, h, stride, False, expand_ratio=6)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GhostModule(\n  (primary_conv): Sequential(\n    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (cheap_operation): Sequential(\n    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Ghost\n",
    "input_channel=64\n",
    "output_channel=64\n",
    "k=2\n",
    "s=2\n",
    "se_ratio=2\n",
    "gho = GhostModule(input_channel, output_channel, kernel_size=1, ratio=2, dw_size=3, stride=2, relu=True)\n",
    "gho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GhostBottleneck(\n  (ghost1): GhostModule(\n    (primary_conv): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (cheap_operation): Sequential(\n      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (ghost2): GhostModule(\n    (primary_conv): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sequential()\n    )\n    (cheap_operation): Sequential(\n      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sequential()\n    )\n  )\n  (shortcut): Sequential()\n)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ghobotto = GhostBottleneck(64, 64, 64, dw_kernel_size=3,\n",
    "                 stride=1, act_layer=nn.ReLU, se_ratio=0.)\n",
    "ghobotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n"
    }
   ],
   "source": [
    " cfgs = [\n",
    "        # k, t, c, SE, s \n",
    "        # stage1\n",
    "        [[3,  16,  16, 0, 1]],\n",
    "        # stage2\n",
    "        [[3,  48,  24, 0, 2]],\n",
    "        [[3,  72,  24, 0, 1]],\n",
    "        # stage3\n",
    "        [[5,  72,  40, 0.25, 2]],\n",
    "        [[5, 120,  40, 0.25, 1]],\n",
    "        # stage4\n",
    "        [[3, 240,  80, 0, 2]],\n",
    "        [[3, 200,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 480, 112, 0.25, 1],\n",
    "         [3, 672, 112, 0.25, 1]\n",
    "        ],\n",
    "        # stage5\n",
    "        [[5, 672, 160, 0.25, 2]],\n",
    "        [[5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1],\n",
    "         [5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1]\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "print(len(cfgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitentranceconda9b4673bfa1754abd8ca93decff70e970",
   "display_name": "Python 3.7.0 64-bit ('entrance': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}