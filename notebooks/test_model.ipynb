{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X1 shape: torch.Size([1, 16, 1, 1])\nX2 shape: torch.Size([1, 32, 1, 1])\nX3 shape: torch.Size([1, 128, 1, 1])\nShape fea:  torch.Size([1, 64, 28, 28])\nRemainig X:  torch.Size([1, 196])\n"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../models\")\n",
    "from pfld import InvertedResidual\n",
    "from ghostnet import GhostBottleneck, GhostModule, GhostNet, ghostnet, _make_divisible, ConvBnAct\n",
    "from pfld import PFLDInference, conv_bn\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "cfgs = [\n",
    "        # k, t, c, SE, s \n",
    "        # stage1\n",
    "        [[3,  16,  16, 0, 2]],\n",
    "        # stage2\n",
    "        [[3,  48,  24, 0, 1]],\n",
    "        [[3,  72,  24, 0, 1]],\n",
    "        # stage3\n",
    "        [[5,  72,  40, 0.25, 1]],\n",
    "        [[5, 120,  40, 0.25, 1]],\n",
    "        # stage4\n",
    "        [[3, 240,  64, 0, 1]], #The original number of channels here is 80, but I change to 64 so that it fit to the AuxiliaryNet\n",
    "        [[3, 200,  80, 0, 2],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 480, 112, 0.25, 1],\n",
    "         [3, 672, 112, 0.25, 1]\n",
    "        ],\n",
    "        # stage5\n",
    "        # [[5, 672, 160, 0.25, 1]],\n",
    "        # [[5, 960, 160, 0, 1],\n",
    "        #  [5, 960, 160, 0.25, 1],\n",
    "        #  [5, 960, 160, 0, 1],\n",
    "        #  [5, 960, 160, 0.25, 1]\n",
    "        # ],\n",
    "\n",
    "        # final\n",
    "        [[5, 16, 16, 0.25, 1]]\n",
    "    ]\n",
    "\n",
    "class CustomizedGhostNet(nn.Module):\n",
    "    def __init__(self, cfgs, width=1.0, dropout=0.2):\n",
    "        super(CustomizedGhostNet, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # building first layer\n",
    "        output_channel = _make_divisible(16 * width, 4)\n",
    "        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channel)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        input_channel = output_channel\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        first_6_stages = []  # This one used for another branch\n",
    "        remaining_stages = []\n",
    "        stages = []\n",
    "        block = GhostBottleneck\n",
    "        for i, cfg in enumerate(self.cfgs):\n",
    "            layers = []\n",
    "            for k, exp_size, c, se_ratio, s in cfg:\n",
    "                output_channel = _make_divisible(c * width, 4)\n",
    "                hidden_channel = _make_divisible(exp_size * width, 4)\n",
    "                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n",
    "                            se_ratio=se_ratio))\n",
    "                input_channel = output_channel\n",
    "\n",
    "            if i<=5:\n",
    "                first_6_stages.append(nn.Sequential(*layers))\n",
    "            else:\n",
    "                remaining_stages.append(nn.Sequential(*layers))\n",
    "                \n",
    "\n",
    "        output_channel = _make_divisible(exp_size * width, 4)\n",
    "        remaining_stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n",
    "        input_channel = output_channel\n",
    "        \n",
    "        self.begining_blocks = nn.Sequential(*first_6_stages)\n",
    "        self.remaining_blocks = nn.Sequential(*remaining_stages)  # 16x14x14\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv7 = conv_bn(16, 32, 3, 2)  # [32, 7, 7]\n",
    "        self.conv8 = nn.Conv2d(32, 128, 7, 1, 0)  # [128, 1, 1]\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.avg_pool1 = nn.AvgPool2d(14)\n",
    "        self.avg_pool2 = nn.AvgPool2d(7)\n",
    "        self.fc = nn.Linear(176, 196)\n",
    "\n",
    "        # building last several layers\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        features_for_auxiliarynet = self.begining_blocks(x)\n",
    "        x = self.remaining_blocks(features_for_auxiliarynet)\n",
    "\n",
    "        x1 = self.avg_pool1(x)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x2 = self.avg_pool2(x)\n",
    "\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "\n",
    "        x3 = self.relu(self.conv8(x))\n",
    "\n",
    "        x3 = x3.view(x1.size(0), -1)\n",
    "\n",
    "        multi_scale = torch.cat([x1, x2, x3], 1)\n",
    "        landmarks = self.fc(multi_scale)\n",
    "    \n",
    "        return features_for_auxiliarynet, landmarks\n",
    "\n",
    "\n",
    "model = CustomizedGhostNet(cfgs)\n",
    "model.eval()\n",
    "input = torch.randn(1,3,112,112)\n",
    "fea, x = model(input)\n",
    "print(\"Shape fea: \", fea.shape)\n",
    "print(\"Remainig X: \", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Feature after ghotnet block:  torch.Size([1, 960, 4, 4])\nFeature after global pool:  torch.Size([1, 960, 1, 1])\nFeature after conv_head:  torch.Size([1, 1280, 1, 1])\nTime inference one input: 0.038233280181884766\ntorch.Size([1, 1000])\nOut1 shape: torch.Size([1, 64, 28, 28])\nX1 shape: torch.Size([1, 16, 1, 1])\nX2 shape: torch.Size([1, 32, 1, 1])\nX3 shape: torch.Size([1, 128, 1, 1])\nTime:  0.03032398223876953\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = ghostnet(width=1)\n",
    "model.eval()\n",
    "# print(model)\n",
    "input = torch.randn(1,3,112,112)\n",
    "\n",
    "t1 = time.time()\n",
    "y = model(input)\n",
    "print(\"Time inference one input:\", time.time()-t1)\n",
    "print(y.size())\n",
    "\n",
    "input = torch.randn(1, 3, 112, 112)\n",
    "plfd_backbone = PFLDInference()\n",
    "plfd_backbone.eval()\n",
    "t1 = time.time()\n",
    "features, landmarks = plfd_backbone(input)\n",
    "print(\"Time: \", time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual\n",
    "w=64\n",
    "h=64\n",
    "stride=2\n",
    "expand_ratio=1\n",
    "res = InvertedResidual(w, h, stride, False, expand_ratio=6)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GhostModule(\n  (primary_conv): Sequential(\n    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (cheap_operation): Sequential(\n    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Ghost\n",
    "input_channel=64\n",
    "output_channel=64\n",
    "k=2\n",
    "s=2\n",
    "se_ratio=2\n",
    "gho = GhostModule(input_channel, output_channel, kernel_size=1, ratio=2, dw_size=3, stride=2, relu=True)\n",
    "gho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GhostBottleneck(\n  (ghost1): GhostModule(\n    (primary_conv): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (cheap_operation): Sequential(\n      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (ghost2): GhostModule(\n    (primary_conv): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sequential()\n    )\n    (cheap_operation): Sequential(\n      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sequential()\n    )\n  )\n  (shortcut): Sequential()\n)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ghobotto = GhostBottleneck(64, 64, 64, dw_kernel_size=3,\n",
    "                 stride=1, act_layer=nn.ReLU, se_ratio=0.)\n",
    "ghobotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n"
    }
   ],
   "source": [
    " cfgs = [\n",
    "        # k, t, c, SE, s \n",
    "        # stage1\n",
    "        [[3,  16,  16, 0, 1]],\n",
    "        # stage2\n",
    "        [[3,  48,  24, 0, 2]],\n",
    "        [[3,  72,  24, 0, 1]],\n",
    "        # stage3\n",
    "        [[5,  72,  40, 0.25, 2]],\n",
    "        [[5, 120,  40, 0.25, 1]],\n",
    "        # stage4\n",
    "        [[3, 240,  80, 0, 2]],\n",
    "        [[3, 200,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 184,  80, 0, 1],\n",
    "         [3, 480, 112, 0.25, 1],\n",
    "         [3, 672, 112, 0.25, 1]\n",
    "        ],\n",
    "        # stage5\n",
    "        [[5, 672, 160, 0.25, 2]],\n",
    "        [[5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1],\n",
    "         [5, 960, 160, 0, 1],\n",
    "         [5, 960, 160, 0.25, 1]\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "print(len(cfgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitentranceconda9b4673bfa1754abd8ca93decff70e970",
   "display_name": "Python 3.7.0 64-bit ('entrance': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}